<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8">
    <title>An Introduction to IIIF</title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <link rel="stylesheet" href="assets/build/css/styles.css" />
    <link href='https://fonts.googleapis.com/css?family=Arimo:400,400i,700,700i' rel='stylesheet' type='text/css'>
    <script src="assets/build/js/vendor/modernizr-output.js"></script>
    <script src="https://use.typekit.net/xtb7ulc.js"></script>
    <script>
      try {
        Typekit.load({
          async: true
        });
      } catch (e) {}
    </script>
    <link rel="stylesheet" href="assets/css/iiif.css" />
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="assets/build/js/vendor/jquery.scrolly.js"></script>
    <script src="assets/build/js/vendor/prism.js"></script>
    <script src="openseadragon/openseadragon.min.js"></script>
    <link rel="apple-touch-icon" sizes="180x180" href="assets/build/img/favicons/apple-touch-icon.png">
    <link rel="icon" type="image/png" href="assets/build/img/favicons/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="assets/build/img/favicons/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="assets/build/img/favicons/manifest.json">
    <link rel="mask-icon" href="assets/build/img/favicons/safari-pinned-tab.svg" color="#00d0b8">
    <link rel="shortcut icon" href="assets/build/img/favicons/favicon.ico">
    <meta name="apple-mobile-web-app-title" content="Digirati">
    <meta name="application-name" content="Digirati">
    <meta name="msapplication-config" content="assets/build/img/favicons/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
  </head>
  <body class="static-site">
    <div class="container">
      <section class="top-section ">
        <div class="column">
          <header class="c-header" role="banner">
            <a class="c-header__logo-link" href="home.html">
              <img class="c-header__logo" src="assets/build/img/digirati-logo-white.svg" alt="" />
              <span class="vh">Digirati</span> 
            </a> 
            <div class="c-header__content">
              <h1 class="c-header__heading">An Introduction to IIIF</h1>
            </div>
            <button class="c-menu js-menu" aria-label="Menu" aria-controls="navigation" title="Menu">
              <span class="c-menu__box">
                  <span class="c-menu__inner">Menu</span> 
              </span> 
            </button>
          </header>
        </div>
        <div class="main" role="main">
          <div class="main__inner-wrapper">
            <div class="ss-title">
              <div class="parallax" data-velocity=".5" data-fit="-230" style="opacity: .2; height: 400px; background-image: url('assets/build/img/content/illustrations/about-hero.png')"></div>
              <div class="text">
                <h1>An Introduction to IIIF</h1>
              </div>
            </div>
          </div>
        </div>
      </section>
    </div>
    <main class="" role="main">
      <div class="static-content">
        <div class="copy">
          <h2>Abstract</h2>
          <p>
            <i>The International Image Interoperability Framework (IIIF, pronounced “triple-eye-eff”) is a set of application programming interfaces (APIs) based on open web standards and defined in specifications derived from shared real world use cases.
              It is also a community that implements those specifications in software, both server and client. This article provides a non-technical overview of the standards, and the benefits they bring to the sharing of content.</i>
          </p>
        </div>
      </div>
      <div class="ss-generic parallax" data-velocity=".5" data-fit="-230" style="background-image: url('https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/Schlagwortkatalog.jpg/512px-Schlagwortkatalog.jpg')">
        <div class="copy">
          <h2>Descriptive semantics</h2>
          <p>A library or museum catalogue uses a metadata schema to capture information such as
            <i>creator</i> or
            <i>subject</i>. But it is not a requirement for enjoying a library, gallery or museum that we consult the catalogue first. We can go and look at things, and the curators of the content can arrange the things so as to encourage us to look at them.</p>
          <p>The records in the catalogue describe the things in the collection. Browsing or searching the records can be easier than hunting through books on the shelves. We can find the descriptions of books by a particular author, or descriptions of paintings
            of seascapes. The records comprise the
            <i>descriptive metadata</i> available for each object. The descriptive metadata records are small and easy to share. First on cards, then microfiche, then electronic records.</p>

          <!--<img width="512" alt="Schlagwortkatalog"-->

          <!--src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/Schlagwortkatalog.jpg/512px-Schlagwortkatalog.jpg" />-->
          <br />
          <cite>
            <a href="https://commons.wikimedia.org/wiki/File%3ASchlagwortkatalog.jpg">
              Image credit: Dr. Marcus Gossler (Own work) [GFDL (http://www.gnu.org/copyleft/fdl.html) or CC-BY-SA-3.0 (http://creativecommons.org/licenses/by-sa/3.0/)], via Wikimedia Commons
            </a> 
          </cite>
        </div>
      </div>
      <div class="ss-generic parallax" data-velocity=".1" style="background-image: url('https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/Photograph%3B_the_Wellcome_Institute_Library%2C_1983_Wellcome_L0015799.jpg/512px-Photograph%3B_the_Wellcome_Institute_Library%2C_1983_Wellcome_L0015799.jpg')">
        <div class="copy">
          <h2>Presentation semantics</h2>
          <p>The descriptive scheme provided by metadata is also used in the library to present the real objects to us. Books might be shelved by subject, and then by author. A decision has been made about collecting things together to make it easy for us
            to find them in a physical space. Conventionally, that decision is driven by a metadata schema. There is usually some relationship between the arrangement of items on view (and in storage), and the model that the metadata schema uses to describe
            the world.</p>
          <p>In an exhibtion space, that arrangement may be partly or entirely unconnected to a formal metadata schema, but there has still been a decision made about how objects relate to each other and how they are aggregated in collections for people
            to look at or interact with.</p>

          <!--<img width="512" alt="Photograph; the Wellcome Institute Library, 1983 Wellcome L0015799"-->

          <!--src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/Photograph%3B_the_Wellcome_Institute_Library%2C_1983_Wellcome_L0015799.jpg/512px-Photograph%3B_the_Wellcome_Institute_Library%2C_1983_Wellcome_L0015799.jpg"/>-->
          <br/>
          <cite>
            <a href="https://commons.wikimedia.org/wiki/File%3APhotograph%3B_the_Wellcome_Institute_Library%2C_1983_Wellcome_L0015799.jpg">
              Photograph; the Wellcome Institute Library, 1983 Wellcome L0015799 See page for author [CC BY 4.0 (http://creativecommons.org/licenses/by/4.0)], via Wikimedia Commons</a> 
          </cite>
        </div>
      </div>
      <div class="static-content static-content--light-grey">
        <div class="copy">
          <h2>The Human Presentation API</h2>
          <p>The arrangement of books on shelves, and the design and conventions of book covers, are part of our human API for interacting with the world.</p>
          <img src="assets/img/hazlitt.jpg" alt="Selected Writings" width="512" />
          <br/>
          <cite>Photograph by author</cite>
          <p>When we find a particular book in a library or look at a painting in a gallery, we don’t need to consult a metadata standard to understand what the text and images on the book cover or on a gallery label mean. It’s part of our shared cultural
            understanding of the world. We know the publisher, title and author of this book by looking at the “metadata” on the cover; we can pick it up and read it. If we're looking at a painting or a sculpture, the descriptive metadata might help us
            understand it better, but we never confuse the description with the object itself.</p>
          <img width=512 src="https://s-media-cache-ak0.pinimg.com/736x/f4/f9/de/f4f9de9d8413946c91a97c98beba759a.jpg" alt="Tilda Swinton, the Maybe" />
          <br/>
          <cite>
            <a href="https://uk.pinterest.com/livingpractice/tilda-swinton/">Via Pinterest. TBC</a> 
          </cite>
          <p>We know how the strings of text presented to us on the gallery label relate to what we can see in front of us. We don't need a guide or a key to interpret the label.</p>
        </div>
      </div>
      <div class="static-content">
        <div class="copy">
          <h2>Descriptive semantics give us pathways</h2>
          <p>Just as the descriptive semantics on the cards influence the presentation of the material in a physical space, we can use the metadata to drive navigation. On the web, we have the ability to shelve a book in many different places at once, so
            descriptive metadata gives us powerful tools for exploration. Descriptive semantics inform the collection of books on shelves, the ink-on-paper of a book cover, the labelling of an exhibit or the information architecture of a web site.</p>
          <img src="assets/img/alpha.png" alt="alpha" width="512" />
          <br />
          <cite>TODO</cite>
          <p>These are all perhaps statements of the obvious. I don’t have to consult catalogue metadata to browse, read or view a physical book or painting, or write an essay about it. The Human Presentation API is my cultural awareness of what book covers
            and gallery labels mean. Which way up to hold a book, whether to start at the back or the front, how to turn the pages, how to interpret a table of contents and navigate to a chapter, how to use an index.</p>
        </div>
      </div>
      <div class="static-content">
        <div class="copy">
          <h2>Digital surrogate</h2>
          <p>I still have my cultural awareness when looking at a digital surrogate on screen. But the computer needs assistance in presenting that digital surrogate to me and allowing me to interact with it. The process needs to be assisted by metadata
            to get the right pixels on the screen in the right place, so that my human cultural awareness can take over again. If I'm viewing things over the web, the machines and software involved need help to let me interact with the object.</p>
          <p>People have been putting digital surrogates online since the start of the web. A digitisation project is accompanied by development of a web site to show that collection. Maybe some work is done to make a nice viewer - a page-turner or other
            client application to read books, present multiple views of a statue or artwork, or similar. Some projects have made use of deep zoom technologies and formats like Zoomify and Seadragon DZI, and invested in Image Server technology. When the
            funding for one project finishes, the best that can be hoped for it that it remains online, albeit in its own silo of probably non-interoperable content. Formats and technologies ossify and become obsolete (there are digitised collections
            trapped in Flash viewers). And even when the technologies are still current, the online presentation part of most completed digitisation projects often did not consider how others might later re-use or consume that content. They did not consider
            interoperability other than that afforded by the web itself, at the level of web pages and images. We need something more formal and specific to convey the complex structure of a digitised book or sequence of images. New projects created new
            formats without really thinking about it. The description of a digital object for consumption in a viewer is a problem that has been addressed again and again for project after project.</p>
          <p>
            <i>Some screen shots from digitisation projects, take from Bodleian? arrange in grid?
            </i>
          </p>
          <p>For anyone trying to use the accumulated wealth of digitised resources from around the world, whether for research or personal interest, the lack of standardisation means that each digitised collection needs to be worked with on its own terms.
            While this is necessary and even desirable for descriptive metadata, it does nothing for the content itself. There has been no standardised way of referring to a page of a book, or a sentence in a handwritten letter, from one digitised collection
            to the next. The Descriptive Metadata doesn’t let us refer to parts of the work, down to the tiniest detail - interesting marginalia, a single word on a page - and make statements about those parts in the web of linked data.</p>
        </div>
      </div>
      <div class="static-content">
        <div class="copy">
          <h2>We need a standard!</h2>
          <p>Obviously, a standardised way of describing a digital surrogate would be beneficial. It would mean that my content has a better chance of a longer life, it would mean that I could benefit from the software development efforts of others by adopting
            shared formats. Server software to generate the representations and client software to view it need not be reinvented for every project. It would be good if my digitised books worked in your viewer, and yours worked in my viewer, and we could
            both have the option of picking off-the-shelf viewers as well as building our own. And much more than that - making our digital surrogates interoperable allows others to reuse them in ways we haven't thought of. We need a model for describing
            digital representations of objects and a format for software - viewing tools, annotation clients, web sites - to consume and render the objects and the statements made about them, by us and others. The model needs to be rich enough to accomodate
            composition of all kinds of web resources to enhance, describe and annotate the objects.</p>
          <p>A hypothetical effort at standardisation might go along these lines:</p>
          <ul>
            <li>"So many standards to choose from"</li>
            <li>"And we can always make more!!!"</li>
            <li>"OK, we need to have the pages in the right order"</li>
            <li>"And structure to drive navigation within the book"</li>
            <li>"And metadata to describe the pagination, reading direction"</li>
            <li>"And we want deep zoom images"</li>
            <li>"And we need metadata to show all the things that the user needs to see"
            </li>
            <li>"Like the title and the author and what it’s about"</li>
            <li>"And which of our collections it’s in"</li>
            <li>"And the material, the curator wants the binding material in the model"</li>
            <li>"Does a painting have an author?"</li>
            <li>"Hang on, this problem has been solved already"</li>
            <li>"Let’s have a look at mappings to cool things like CIDOC-CRM..."</li>
          </ul>
          <p>
            This process seemed to start out really well, and progress was made on the requirements for an interoperable standard. But it started geting complex quite quickly, as all the different ways of describing objects begain to bear down on the emerging model.
            Questions like
            <i>"does a painting have an author"</i> don't concern us when we are looking at a painting. We don't need to accomodate that kind of question in our human Presentation API. They only get raised when we're talking about what to put in the catalogue
            records, when we implement a descriptive schema. In the above discussion, the participants are sometimes talking about the actual objects, and sometimes talking about descriptive metadata about the objects.</p>
        </div>
      </div>
      <div class="static-content">
        <div class="copy">
          <h2>The IIIF Presentation API</h2>
          <p>iiif logo</p>
          <p>The IIIF Presentation API avoids this problem by being very clear about what it is for. It's not concerned with descriptive metadata. It has no opinion about whether a painting has an author. Its job is to describe an object for presentation.
            This means getting pixels on screen to drive a viewer, or offering a surface for annotation. The model is about
            <b>presentation semantics</b>
            rather than descriptive semantics. The descriptive metadata are not what will help us paint the pixels on the screen so that we can read the pages, look at the brush strokes or see the film grain. Instead we need information about the images, audio, video,
            text and other content so we can display it; we need information about the structure of the object to navigate it; we need information about pagination, orientation, reading direction and so on, and we need links to other sources of data that
            can help us make sense of the object - web pages, catalogue records in particular formats, etc.</p>
          <p>The IIIF Presentation API provides:
            <ul>
              <li>A model for describing digital representations of objects: "Just enough metadata to drive a remote viewing experience"
              </li>
              <li>A format for software - viewing tools, annotation clients, web sites - to consume and render the objects and the statements made about them in the form of annotations
              </li>
            </ul>
          </p>
        </div>
      </div>
      <div class="static-content">
        <div class="copy">
          <h2>But where's my model?</h2>
          <p>This doesn't mean that the descriptive metadata has no place in a digital object delivered by the Presentation API. It's important that the object is accompanied by useful information, and links to other descriptions of the object. The Presentation
            API takes great care to ensure that you can accompany your digital objects with rich human-readable descriptions, with support for multiple languages, so that viewers can display that important contextual information to users. It also provides
            an explicit mechanism for linking to one or more semantic descriptions of the object depicted, as well as related human-readable resources. There's quite a bit of descriptive metadata being presented here:</p>
          <p>
            <img src="assets/img/uv-tour-wales.jpg" />
            <cite>https://viewer.library.wales/4692237</cite>
          </p>
          <p>For the Presentation API, the meaning of any accompanying descriptive metadata for display in a viewer is irrelevant. The API's job is to get the content of the work - the pages of the book, the painting - to a point where a human can interact
            with it in a logical way. To view it, read it, annotate it, mix it up with other things if they want. A IIIF client can also display any accompanying metadata included as multilingual pairs of labels and values. But that's as far as it goes,
            it needs no definition or scheme for what that metadata means. It is
            <i>outside of the scope</i> of the Presentation API. In the screen shot above, the user can
            <i>view</i> important semantic metadata - but the Presentation API is just a conduit for that metadata. In the Presentation API, those strings have no semantic significance. They are not defined in the specification. A client of the API should
            just render them. If you want to know what they mean, look at the link to descriptive metadata that a publisher of IIIF resources provides.</p>
          <p>The Presentation API is not a new metadata standard to describe your objects. It is not an alternative to or replacement for any existing descriptive metadata standards, because it has a different function. The many rich and various ways in
            which different communities describe objects are diverse for good reason. The models a community or an individual institution adopt to describe the meaning of its objects in the world, from cataloguing schemes to APIs, benefit from shared
            vocabularies and common practice within large communities, but attempts at complete standardisation fail and are not even desirable. The models, standards and APIs an institution adopts for description are an expression of its view of the
            world. However, while it is not reasonable or desirable that everyone describes their objects the same way semantically, it is desirable that an insitution
            <i>presents</i> its objects via a common standard. Many descriptive standards, one Presentation standard - for the same reason that it is sensible to make your web pages compatible with most web browsers.</p>
          <p>This does not mean that the user experience driven by the Presentation API has to be standard; far from it. IIIF is not about standardising the user experience. An object described by the Presentation API could be rendered by a conventional
            bookreader style viewer, loaded into a scholarly workbench application for annotation, displayed as an explosion of thumbnails, projected into a virtual space, rendered as minimalist web pages, remixed into multimedia presentations, worked
            into online exhibitions, reused in physical gallery space or turned into games and interactive experiences. The Presentation API model encourages creative re-use of the content, and to this end ensures it stays separate from the descriptive
            metadata. A Presentation API resource is portable, reusable and interoperable. If you have a digitised resource, you provide a Presentation API resource and a semantic description of the object, and the two link to each other.
          </p>
        </div>
      </div>
      <div class="static-content">
        <div class="copy">
          <h2>Manifests and Canvases</h2>
          <p>How does the Presentation API work?</p>
          <img src="http://iiif.io/api/presentation/2.1/img/objects.png" width="200" style="float:left;margin-right:15px;" />
          <div class="ablock">
            <h3>Manifests for things</h3>
            <p>The Presentation API describes “just enough metadata to drive a remote viewing experience”. This metadata is a
              <b>IIIF Manifest</b>. The Manifest represents the thing. A book. A painting. A film. A sculpture. An opera. A long-playing record. A manuscript. A map. An aural history field recording. A videocassette of a public information film. A laboratory
              notebook. A diary. All of these things would be represented by a IIIF Manifest. A manifest is what you load into a viewer or use to generate a web page. If the object the manifest represents is a photograph, there might only be one conceptually
              distinct view of it that we wish to convey via the Presentation API, to end up on a user's screen. For many objects there is more than one view. Even for a painting, it might be important to include the back of the canvas frame. And for
              books, manuscripts and much archive material, each page, leaf, folio or sheet is one or two separate views - we can't look at all of them at once, the model conveys them as a sequence of distinct views. Depending on how the book has been
              captured and how we want to model it, we might have one view per page, or one view per double page spread, and extra views for inserts or supplementatry material.</p>
            <h3>Canvases for views</h3>
            <p>
              These views are represented by
              <b>Canvases</b>. A Manifest contains one or more
              <b>Sequences</b> of
              <b>Canvases</b>. A canvas is not the same as an image. The canvas is an abstraction, a virtual container for content. It's analogous to a PowerPoint slide; an initially empty container, onto which we "paint" content. In coming up with a model
              for providing a sequence of images to a book reading application, or for viewing paintings, the concept of a canvas may seem like an extra layer of complexity. It's not much more complicated to do it this way, but it is much more flexible
              and powerful.</p>
            <img src="https://camo.githubusercontent.com/5fcc89a4144e15741d7c37863f4ff8d01602f4ab/687474703a2f2f746f6d6372616e652e6769746875622e696f2f736372617463682f696d672f626162656c2d63616e7661732e706e67" />
            <p>
              <i>The canvas is the abstract space; we provide an image to paint the canvas
              </i>
            </p>
            <p>The Canvas keeps the content separate from the conceptual model of the page of the book, or the painting, or the movie. The content can be images, blocks of text, video, links to other resources, and the content can be positioned precisely
              on the canvas. By including a Canvas in a Manifest, you assert a space on which you and others can
              <b>annotate</b> content. For image-based content the PowerPoint analogy is clear: the Canvas is a 2D rectangular space with an aspect ratio. The height and width properties of a canvas define the aspect ratio and provide a simple coordinate
              space. This coordinate space allows the creator of the manifest to associate whole or parts of content with whole or parts of canvases, and for anyone else to make their own annotations in that space. This means that you can provide more
              than one representation of a view. You might have a painting photographed in natural light and in X-ray. You might have a manuscript that was captured to microfilm, and your initial presentation of the material uses images derived from the
              microfilm. Later, you go back and photograph some of the folios at high resolution, maybe those with illuminations. You can update the content associated with a Canvas, without having to retract the canvas and the other content you might
              already have associated with it. You may have a manuscript; in IIIF it is represented as a sequence of Canvases, but for some of those Canvases you have no image at all - the page was known to exist, but is now lost. You may still have text
              content associated with the Canvas - transcriptions from a copy, commentary, or other notes. The fact that for this particular folio you have no photographic representation doesn't stop you modelling it in the Manifest and associating content
              with it - just not a pictorial representation in this case.</p>
          </div>
          <h3>Annotations for content</h3>
          <p>All association of content with a canvas is done by
            <b>annotation</b>. The IIIF Presentation API is built on the Open Annotation standard, which has now become the W3C Web Annotation Data Model. At its simplest, the Web Annotation Data Model is a formalised way of linking resources together:</p>
          <p>
            <i>An annotation is considered to be a set of connected resources, typically including a body and target, and conveys that the body is related to the target. The exact nature of this relationship changes according to the intention of the annotation,
              but the body is most frequently somehow "about" the target. This perspective results in a basic model with three parts, depicted below. The full model supports additional functionality, enabling content to be embedded within the annotation,
              selecting arbitrary segments of resources, choosing the appropriate representation of a resource and providing styling hints to help clients render the annotation appropriately.</i>
          </p>
          <img src="https://www.w3.org/TR/annotation-model/images/intro_model.png" width="400" />
          <p>A simple annotation might be an association between a page of a manuscript and an article about that page elsewhere on the web. Or, in the context of a bookreader or viewer, it might be a comment on or transcription of a particular part of the
            page, or the whole page. This notion of annotations as commentary or transcriptions is familiar:</p>
          <img src="assets/img/image-annos.png" />
          <p>But in IIIF, the image itself is one just of the pieces of content annotating the abstract canvas. There may be multiple images, there may be no images at all. This diagram shows that all the content a user ever sees rendered by a viewer - images,
            text and other content - is associated with the virtual space of the canvas via the mechanism of annotation.</p>
          <img src="assets/img/img-and-canvas-annos.png" />
          <p>IIIF distinguishes between annotations that are for
            <b>painting</b> on to the canvas - images, transcriptions - and other annotations, that don't necessarily make sense rendered directly onto the virtual space. For example, commentary might be rendered alongside the image in a viewer, not superimposed
            on top of it, but transcription could be superimposed directly in a layer that can be toggled on and off:
            <img src="assets/img/transcription.jpg" />
            <cite>http://wellcomelibrary.org/item/b28769454, manifest: http://wellcomelibrary.org/iiif/b28769454/manifest
            </cite>
            <p>When you publish a manifest, you publish a sequence of one or more canvases that are almost always accompanied by one or more image annotations - usually just one. Suppose you have a digitised book, the manifest that represents it consists
              of a sequence of canvases, with image annotations:</p>
            <img src="assets/img/manifest-sequence.png" />
            <p>
              Although in this initial state each canvas is accompanied in the manifest by just one image annotation, the stage is set for you and others to add more annotations in future. When you add annotations, you might publish them in your manifest alongside
              the image annotations. When other people annotateyour content, they can't do this directly. But they can still create annotations using the identity and coordinate system you have extablished for your canvas. This allows make annotations
              on your content for my own private use, or for me to publish them independently and combine them with yout Manifest and its canvases in my own presentation of your material, or even for you to accept my annotations back and incorporate them
              into your published content.</p>
            <p>The canvas establishes a stage in which the simplest case - one image per canvas - is straightforward, but more complex cases, more complex and interesting associations of content, follow naturally. Suppose a manuscript folio that once looked
              like this:</p>
            <img src="assets/img/bod-manus-1.png" />
            <p>...was torn up and its various parts scattered. Today, we have images of three surviving parts:</p>
            <img src="assets/img/mtl.png" width="200" />
            <img src="assets/img/mtr.png" width="200" />
            <img src="assets/img/mbl.png" width="200" />
            <p>We can include a canvas for this missing leaf, and annotate the three parts we do have onto it, as well as providing some commentary about this missing piece:</p>
            <img src="assets/img/fragments.png" />
            <p>Again, the similarity between this and a PowerPoint slide is noticeable. But unlike a Powerpoint slide, the Manifest, the Canvas and all the annotations of content onto it, are interoperable, and part of the web of linked data. We and anyone
              else can make statements about them, and add to the statements about them, in the web of linked data. And we publish all this information in easy to consume, interopeable API for ourselves and others to view, interact with, and build new
              interesting things from.</p>
        </div>
      </div>
      <div class="static-content">
        <div class="copy">
          <h2>Canvases and 2D space</h2>
          <p>The IIIF Presentation API isn't just for images. So far we have looked at 2D canvases. The current IIIF Presentation API has supported 2D canvases right from the start. While we can annotate time-based media (audio and video) onto a 2D Canvas
            in space, in exactly the same way we position images and text onto the canvas using the coordinate system, the current published specification doesn't allow us to annotate images, text, video, audio and other content onto the canvas at a particular
            point in time, because a Canvas has no duration, no concept of a time dimension to accompany the width and height. If we add a time dimension, it allows us to annotate in both space and time:
            <p>FIRE HERE</p>
            <p>
              And also, we can have a Canvas that has only a duration, no width or height - onto which we could annotate audio tracks. The same model accomodates this. In the future, the model may also accomodate 3D or 3D time-based media - but the focus for this year
              is to make the model work well for AV, just as it does for image and text content.
            </p>
        </div>
      </div>
      <div class="static-content">
        <div class="copy">
          <h2>The Image API</h2>
          <p>So far, we have looked at annotating images onto canvases. A manifest contains a sequence of one or more canvases, each of which might have an image and possibly additional content - more images, text annotations, commentary, transcriptions
            and so on. While single static images are enough to drive a viewing experience, for many objects of cultural heritage having just one image representation available is not very flexible. Typically manuscripts, artworks, maps and to some extent
            books and archive material are digitised at a very high resolution. We need access to that high resolution to view the detail in an image, but requiring our clients to load multi-mega- or even giga-pixel images just to look at book pages is
            unreasonable. We need to have a variety of sizes available for different purposes. And ideally, we can choose the regions and sizes we want to use from the image dynamically.</p>
          <p>
            The Image API specifies a syntax for web requests that lets us ask for images at different sizes and in different formats and qualities. Each image endpoint is a web service that returns new derivative images. We can ask for the full image, or regions
            of the image. We can rotate, scale, distort, crop and mirror the whole image, or parts of the image. </p>
          <input id="irb1" type="radio" name="imgapi1" data-region="full" data-size="800," data-rotation="0" data-quality="default" data-format="jpg" />
          <label for="irb1">The whole painting confined to a box 800 pixels wide</label>
          <br />
          <input id="irb2" type="radio" name="imgapi1" data-region="4150,1550,1600,800" data-size="800,400" data-rotation="0" data-quality="default" data-format="jpg" />
          <label for="irb2">A particular rectangular region of the image at a particular size
          </label>
          <br />
          <input id="irb3" type="radio" name="imgapi1" data-region="4150,1550,1600,800" data-size="600,600" data-rotation="0" data-quality="default" data-format="jpg" />
          <label for="irb3">The same region, distorted to 600 x 600</label>
          <br />
          <input id="irb4" type="radio" name="imgapi1" data-region="4150,1550,1600,800" data-size="!600,600" data-rotation="0" data-quality="default" data-format="jpg" />
          <label for="irb4">The same region, confined to 600 x 600</label>
          <br />
          <input id="irb5" type="radio" name="imgapi1" data-region="4150,1550,1600,800" data-size="!600,600" data-rotation="90" data-quality="default" data-format="jpg" />
          <label for="irb5">...and then rotated</label>
          <br />
          <input id="irb6" type="radio" name="imgapi1" data-region="4150,1550,1600,800" data-size="!600,600" data-rotation="!0" data-quality="default" data-format="jpg" />
          <label for="irb6">...or mirrored</label>
          <br />
          <input id="irb7" type="radio" name="imgapi1" data-region="full" data-size="800," data-rotation="0" data-quality="gray" data-format="jpg" />
          <label for="irb7">The whole painting in grayscale</label>
          <br />
          <div id="imgApiUrl">
            https://(image-identifier) /
            <input type="text" style="width:10em" disabled value="region" /> /
            <input type="text" style="width:6em" disabled value="size" /> /
            <input type="text" style="width:4em" disabled value="rotation" /> /
            <input type="text" style="width:6em" disabled value="quality" /> / .jpg
            <br /> https://(image-identifier) /
            <input type="text" class="imgapipart" id="imgRegion" style="width:10em" /> /
            <input type="text" class="imgapipart" id="imgSize" style="width:6em" /> /
            <input type="text" class="imgapipart" id="imgRotation" style="width:4em" /> /
            <input type="text" class="imgapipart" id="imgQuality" style="width:6em" /> / .jpg
            <input type="button" id="imgApiGo" value="go.." />
          </div>
          <img id="imgApiTarget" data-info="https://dlcs.io/iiif-img/3/2/04fbbb28-d5a7-4408-b7da-800c4e65eda3" src="https://dlcs.io/iiif-img/3/2/04fbbb28-d5a7-4408-b7da-800c4e65eda3/full/800,/0/default.jpg" />
          <script>
            $(function() {
              $("input[type=radio][name=imgapi1]").change(function() {
                var $img = $("#imgApiTarget");
                var region = $(this).attr("data-region");
                var size = $(this).attr("data-size");
                var rotation = $(this).attr("data-rotation");
                var quality = $(this).attr("data-quality");
                var src = $img.attr("data-info") + "/" + region + "/" + size + "/" + rotation + "/" + quality + ".jpg";
                $("#imgRegion").val(region);
                $("#imgSize").val(size);
                $("#imgRotation").val(rotation);
                $("#imgQuality").val(quality);
                $img.attr("src", src);
              });
              $("#irb1").click();
              $("#imgApiGo").click(function() {
                var $img = $("#imgApiTarget");
                var src = $img.attr("data-info") + "/" + $("#imgRegion").val() + "/" + $("#imgSize").val() + "/" + $("#imgRotation").val() + "/" + $("#imgQuality").val() + ".jpg";
                $img.attr("src", src);
              });
            });
          </script>
          <p>
            If you provide a IIIF Image API endpoint, you are providing a service that viewers can call to get images. This might be a single static image - you could use the image service to link to an image that’s just the right size for your blog post, using appropriate
            values in the URL as in the examples above and avoid having to make any new derivatives yourself. This use of the Image API works with any browser that supports images.</p>
          <p>
            Or it might be more complex - a deep zoom viewer works by making many requests for small regions of the image, known as tiles. In the following images, we see an exploded view of the tiles the client is requesting from the Image API. It is asking for
            small square regions at a particular size and zoom level. Just as you don’t need to load the whole world into Google Maps to view your neighbourhood at street level detail, a deep zoom client doesn’t need to load an enormous image for you
            to view a particular part of it at the highest resolution. IIIF enables images of Gigapixel size to be viewed in great detail while keeping bandwidth use to the minimum, because the server is able to return small, fast tiles from which the
            viewer can compose the scene.
          </p>
          <img src="assets/img/exploded.png" />
          <p>You can experiment with tiles using
            <a href="http://tomcrane.github.io/presentations/tile-exploder.html">this tool
            </a> .
          </p>
          <p>
            if you have a IIIF Image API endpoint available for each of your images, you have flexibility for how you and others use your image resources. You could, for example, have a static image that turns into a deep zoom image when clicked. Although the image
            below is a regular image tag that works in all browsers, there is some JavaScript enhancement that turns it into a deep zoom image when clicked. In this case it uses the OpenSeadragon library which is used for deep zoom by viewers such as
            the Universal Viewer and Mirador:</p>
          <script src="https://tomcrane.github.io/iiif-img-tag/js/jquery.iiif-image-tag.js"></script>
          <script>
            $(function() {
              $(".iiif-image").iiifImage();
            });
          </script>
          <img class="iiif-image" src="https://dlcs.io/iiif-img/2/1/e75d37c5-40a1-48a4-b61b-52ac7aa26849/full/500,/0/default.jpg" />
          <p>This technique can be extended to build the simplest of manifest viewers:</p>
          <div class="thumbset">
            <img class="iiif-image" src="https://dlcs.io/iiif-img/wellcome/1/3bde01c5-95fc-4c4c-aaaa-8a10f9a0ffc8/full/125,200/0/default.jpg" />
            <img class="iiif-image" src="https://dlcs.io/iiif-img/wellcome/1/9ba139ed-578c-483c-bc71-f093bcd357f5/full/125,200/0/default.jpg" />
            <img class="iiif-image" src="https://dlcs.io/iiif-img/wellcome/1/7c9b7817-e519-44ef-a724-487a00599fc5/full/125,200/0/default.jpg" />
            <img class="iiif-image" src="https://dlcs.io/iiif-img/wellcome/1/e33add4e-6fe8-43de-8aed-f7c128d051ac/full/125,200/0/default.jpg" />
            <img class="iiif-image" src="https://dlcs.io/iiif-img/wellcome/1/b763a277-0067-4398-9ad5-8b946fbe2533/full/125,200/0/default.jpg" />
            <img class="iiif-image" src="https://dlcs.io/iiif-img/wellcome/1/aea8e1ac-b1ba-41cd-b56f-b1acae113de0/full/125,200/0/default.jpg" />
            <img class="iiif-image" src="https://dlcs.io/iiif-img/wellcome/1/80890c4d-d8d0-4f69-b6d9-ebc411e17c0a/full/125,200/0/default.jpg" />
            <img class="iiif-image" src="https://dlcs.io/iiif-img/wellcome/1/96654cec-c4ef-44fe-9e30-8c342767fbdd/full/125,200/0/default.jpg" />
            <img class="iiif-image" src="https://dlcs.io/iiif-img/wellcome/1/ff64989e-4096-4054-9ef1-14ac3d7f1afe/full/125,200/0/default.jpg" />
          </div>
          <p>Another use of an image server is for generating responsive images without having to create multiple derivatives in an image editing application. The following is a contrived example:</p>
          <pre>   
                    <code class="language-html">
&lt;picture&gt;
    &lt;source media="(min-width: 1600px)" srcset="https://dlcs.io/.../120,850,2100,2000/1600,/0/default.jpg"&gt;
    &lt;source media="(min-width: 700px)"  srcset="https://dlcs.io/.../250,850,1950,2000/1000,/0/default.jpg"&gt;
    &lt;source media="(min-width: 400px)"  srcset="https://dlcs.io/.../250,850,900,1000/500,/0/default.jpg"&gt;
    &lt;img alt="example image"               src="https://dlcs.io/.../250,850,900,1000/320,/0/default.jpg"&gt;
&lt;/picture&gt;
                    </code>
</pre>
          <p>See more at
            <a href="https://tomcrane.github.io/iiif-img-tag/">image tag</a> 
          </p>
          <p>See extracted Dublin tool at
            <a href="TODO">here</a> 
          </p>
        </div>
      </div>
      <div class="static-content">
        <div class="copy">
          <h2>Combining deep zoom tile sources and Canvases</h2>
          <p>Once scenario in which multiple images annotate the same canvas is when there is a choice of views of an object. In the following examples, two different user interface treatments are offered for the choice between the natural light and the
            x-ray views of this painting.</p>
          <h3>Side by side using Leaflet.js</h3>
          <iframe sandbox="allow-popups allow-scripts allow-forms allow-same-origin" src="dee-sbs.html" marginwidth="0" marginheight="0" style="height:500px;width:800px;" scrolling="no"></iframe>
          <p>
            <a href="dee-sbs.html" target="_blank">(open in new window)</a> 
          </p>
          <h3>One at a time using OpenSeadragon</h3>
          <div id="deeOsd1" style="width:800px; height:500px"></div>
          <script>
            var deeOsdViewer = OpenSeadragon({
              id: "deeOsd1",
              prefixUrl: "openseadragon/images/"
            });
            deeOsdViewer.addTiledImage({
              tileSource: "https://dlcs.io/iiif-img/3/2/04fbbb28-d5a7-4408-b7da-800c4e65eda3/info.json"
            });
            deeOsdViewer.addTiledImage({
              tileSource: "https://dlcs.io/iiif-img/3/2/8034eb5b-9c90-4471-ad68-52124232ec0c/info.json",
              opacity: 0
            });
            $(document).ready(function() {
              $("input[type='radio']").click(function() {
                deeOsdViewer.world.getItemAt(0).setOpacity(0);
                deeOsdViewer.world.getItemAt(1).setOpacity(0);
                var checked = $(this).is(":checked");
                var index = $(this).attr("data-index");
                var ti = deeOsdViewer.world.getItemAt(index);
                console.log(index);
                if (checked) {
                  ti.setOpacity(1);
                }
              });
            });
          </script>
          <p>
            <input type="radio" name="choices" class="choice" data-index="0" checked id="c0" />
            <label for="c0">Natural light</label>
            <br />
            <input type="radio" name="choices" class="choice" data-index="1" id="c1" />
            <label for="c1">X-ray</label>
          </p>
        </div>
      </div>
      <div class="static-content static-content--green">
        <div class="copy">
          <h2>More IIIF stories</h2>
          <p>But where's my model? IIIF and the rest of the world</p>
          <p>...</p>
        </div>
      </div>
    </main>

    <!--<footer class="c-footer" role="contentinfo">
  <div class="c-footer__logo-wrapper">
    <img class="c-footer__logo" src="assets/build/img/digirati-logo-white-green.svg" alt="Digirati logo" />
  </div>
  <div class="c-contact">
    <div class="c-contact__info c-contact__module">
      <h3 class="c-contact__heading">Contact</h3>
      <a class="c-contact__tel" href="tel:08456434370">0845 643 4370</a> <br />
      <a class="c-contact__email" href="mailto:contact@digirati.com">contact@digirati.com</a> 
      <ul class="c-social">
        <li class="c-social__item">
          <a class="c-social__link c-social__link--linkedin" href="http://www.linkedin.com" target="_blank">
            Connect with us on LinkedIn
            <span class="c-social__image">
              <span class="c-icon c-icon--linked-in">
                  <svg class="c-icon__svg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                    <path fill-rule="evenodd" d="M20.4.01H3.6c-.958 0-1.877.382-2.553 1.06C.372 1.75-.005 2.672 0 3.63v16.75c-.005.958.372 1.88 1.047 2.56C1.723 23.618 2.642 24 3.6 24h16.8c.958 0 1.877-.382 2.553-1.06.675-.68 1.052-1.602 1.047-2.56V3.63c.005-.958-.372-1.88-1.047-2.56C22.277.392 21.358.01 20.4.01zM7.3 20.15H3.7V9.3h3.6v10.85zM5.5 7.82h-.1c-.71.044-1.386-.317-1.744-.933-.358-.616-.338-1.382.053-1.978.39-.598 1.08-.923 1.79-.84.517-.04 1.03.142 1.41.497.38.355.592.852.59 1.372 0 .52-.216 1.015-.595 1.37-.378.357-.887.54-1.406.51zm14.8 12.33h-3.6v-5.81c0-1.46-.6-2.45-1.9-2.45-.83-.015-1.565.525-1.8 1.32-.088.284-.122.583-.1.88v6.06H9.3V9.3h3.6v1.53c.627-1.17 1.876-1.867 3.2-1.79 2.4 0 4.2 1.55 4.2 4.89v6.22z"/>
                  </svg>
              </span> 
            </span> 
          </a> 
        </li>
        <li class="c-social__item">
          <a class="c-social__link c-social__link--twitter" href="https://twitter.com/digirati_uk" target="_blank">
            Connect with us on Twitter
            <span class="c-social__image">
              <span class="c-icon c-icon--twitter">
                  <svg class="c-icon__svg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 20">
                    <path fill-rule="evenodd" d="M24 2.37c-.888.405-1.832.67-2.8.79 1.032-.626 1.784-1.625 2.1-2.79-.95.593-2 1.007-3.1 1.22C19.275.58 17.97.004 16.6 0c-2.747.044-4.94 2.303-4.9 5.05-.017.386.016.773.1 1.15C7.826 5.992 4.14 4.065 1.7.92c-.482.758-.726 1.642-.7 2.54.004 1.675.825 3.243 2.2 4.2-.776-.013-1.535-.23-2.2-.63v.07c-.018 2.36 1.602 4.416 3.9 4.95-.423.117-.86.174-1.3.17-.302 0-.604-.03-.9-.09.598 2.052 2.463 3.475 4.6 3.51-2.053 1.65-4.685 2.405-7.3 2.09 2.228 1.465 4.833 2.254 7.5 2.27 9.1 0 14.1-7.69 14.1-14.37.015-.22-.02-.443-.1-.65.98-.717 1.826-1.6 2.5-2.61z"/>
                  </svg>
              </span> 
            </span> 
          </a> 
        </li>
      </ul>
    </div>
    <div class="c-address c-address--footer c-contact__module vcard">
      <h3 class="c-address__heading fn">London</h3>
      <p class="c-address__address adr">
        <span class="c-address__street street-address">
          <span class="c-address__address-line">Dunstan House</span> 
          <span class="c-address__address-line">14A St Cross St</span> 
        </span> 
        <span class="c-address__locality locality">London</span> 
        <span class="c-address__postal-code postal-code">EC1N 8XA</span> 
        <a class="c-address__link" href="https://www.google.co.uk/maps/place/Dunstan+House,+14A+St+Cross+St,+London+EC1N+8XA/@51.5209335,-0.1094515,17z/data=!3m1!4b1!4m5!3m4!1s0x48761b4e6d4226fd:0xc723a9f5e86f227c!8m2!3d51.5209335!4d-0.1072628" target="_blank">Map</a> 
      </p>
    </div>
    <div class="c-address c-address--footer c-contact__module vcard">
      <h3 class="c-address__heading fn">Glasgow</h3>
      <p class="c-address__address adr">
        <span class="c-address__street street-address">
          <span class="c-address__address-line">70 Pacific Quay</span> 
          
        </span> 
        <span class="c-address__locality locality">Glasgow</span> 
        <span class="c-address__postal-code postal-code">G51 1DZ</span> 
        <a class="c-address__link" href="https://www.google.co.uk/maps/place/Digirati/@55.8570461,-4.2972752,17z/data=!3m1!4b1!4m5!3m4!1s0x48884679df0082b7:0x60405fdac8752bb2!8m2!3d55.8570431!4d-4.2950865" target="_blank">Map</a> 
      </p>
    </div>
    <div class="c-copyright c-contact__module">
      <p class="c-copyright__text">&copy; 2016 Digirati <br/>All rights reserved<br/> <a class="c-copyright__link" href="#">Legals</a> </p>
    </div>
  </div>
</footer>
-->
    <script src="assets/build/js/script.js"></script>
  </body>
</html>